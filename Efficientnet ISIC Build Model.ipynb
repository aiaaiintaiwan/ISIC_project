{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":["!pip install -q efficientnet"],"execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":["import os, re, time, tqdm\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from sklearn import metrics, model_selection\n","\n","import tensorflow as tf\n","import tensorflow_addons as tfa\n","from tensorflow import keras \n","from tensorflow.keras import backend as K\n","from efficientnet import tfkeras as efnet\n","\n","from kaggle_datasets import KaggleDatasets"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["# USE DIFFERENT SEED FOR DIFFERENT STRATIFIED KFOLD\n","SEED = 15\n","tf.random.set_seed(SEED)\n","np.random.seed(SEED)\n","\n","# \n","#FOLDS = 5\n","\n","FOLDS = 10\n","INCLUDE_2019 = 1\n","INCLUDE_2018 = 1\n","INCLUDE_MALIGNANT = 0\n","\n","# DATA PARAMS\n","IMG_READ_SIZE     = 384\n","IMG_SIZE          = 384\n","BALANCE_POS_RATIO = 0.01\n","\n","# MODEL PARAMS\n","EFF_NET      = 0\n","# loss and loss params\n","LOSS_PARAMS  = dict(label_smoothing=0.09)\n","\n","# TRAINING PARAMS\n","BATCH_SIZE  = 32\n","EPOCHS      = 12\n","\n","\n","# VALID AND TEST PARAMS\n","TBM        = 6\n","TTA        = 15\n","VALID_FREQ = 1\n"],"execution_count":null,"outputs":[]},{"source":["## Hardware Setting"],"cell_type":"markdown","metadata":{}},{"metadata":{"trusted":true},"cell_type":"code","source":["DEVICE = \"TPU\"\n","print(\"connecting to TPU...\")\n","try:\n","    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n","    print('Running on TPU ', tpu.master())\n","except ValueError:\n","    print(\"Could not connect to TPU\")\n","    tpu = None\n","if tpu:\n","    try:\n","        print(\"initializing  TPU ...\")\n","        tf.config.experimental_connect_to_cluster(tpu)\n","        tf.tpu.experimental.initialize_tpu_system(tpu)\n","        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n","        print(\"TPU initialized\")\n","    except _:\n","        print(\"failed to initialize TPU\")\n","else:\n","    DEVICE = \"GPU\"\n","\n","if DEVICE != \"TPU\":\n","    print(\"Using default strategy for CPU and single GPU\")\n","    strategy = tf.distribute.get_strategy()\n","\n","if DEVICE == \"GPU\":\n","    print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n","    \n","AUTO              = tf.data.experimental.AUTOTUNE\n","REPLICAS          = strategy.num_replicas_in_sync\n","GLOBAL_BATCH_SIZE = BATCH_SIZE * REPLICAS\n","print(\"REPLICAS: %d\" % REPLICAS)"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["## Preprocessing"]},{"metadata":{},"cell_type":"markdown","source":["https://www.kaggle.com/c/siim-isic-melanoma-classification/discussion/169139"]},{"metadata":{"trusted":true},"cell_type":"code","source":["GCS_PATH1 = KaggleDatasets().get_gcs_path('melanoma-%ix%i' % (IMG_READ_SIZE, IMG_READ_SIZE))\n","GCS_PATH2 = KaggleDatasets().get_gcs_path('isic2019-%ix%i' % (IMG_READ_SIZE, IMG_READ_SIZE))\n","GCS_PATH3 = KaggleDatasets().get_gcs_path('malignant-v2-%ix%i' % (IMG_READ_SIZE, IMG_READ_SIZE))"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["df_base_train = pd.read_csv(\"../input/siim-isic-melanoma-classification/train.csv\")\n","\n","df_base_test = pd.read_csv(\"../input/siim-isic-melanoma-classification/test.csv\")\n","df_base_train.shape"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["train_files = tf.io.gfile.glob(os.path.join(GCS_PATH1, \"train*.tfrec\"))\n","print(len(train_files))\n","#\n","if INCLUDE_2019:\n","    train_files += tf.io.gfile.glob([os.path.join(GCS_PATH2, \"train%.2i*.tfrec\" % i) for i in range(1, 30, 2)])\n","#     print(train_files.shape)\n","if INCLUDE_2018:\n","    train_files += tf.io.gfile.glob([os.path.join(GCS_PATH2, \"train%.2i*.tfrec\" % i) for i in range(0, 30, 2)])\n","#     print(train_files.shape)\n","#\n","if INCLUDE_MALIGNANT:\n","    train_files += tf.io.gfile.glob([os.path.join(GCS_PATH3, \"train%.2i*.tfrec\" % i) for i in range(15, 30, 1)])\n","#     print(train_files.shape)\n","print(\"%d train files found\" % len(train_files))"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["test_files = tf.io.gfile.glob(os.path.join(GCS_PATH1, \"test*.tfrec\"))\n","print(\"%d test files found\" % len(test_files))"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["## Data pipeline"]},{"metadata":{"trusted":true},"cell_type":"code","source":["def read_labeled_tfrecord(example):\n","    tfrec_format = {\n","        'image'                        : tf.io.FixedLenFeature([], tf.string),\n","        'image_name'                   : tf.io.FixedLenFeature([], tf.string),\n","        'target'                       : tf.io.FixedLenFeature([], tf.int64)\n","    }           \n","    example = tf.io.parse_single_example(example, tfrec_format)\n","    return example['image'], example['target']\n","\n","\n","def read_unlabeled_tfrecord(example, return_image_name=True):\n","    tfrec_format = {\n","        'image'                        : tf.io.FixedLenFeature([], tf.string),\n","        'image_name'                   : tf.io.FixedLenFeature([], tf.string),\n","    }\n","    example = tf.io.parse_single_example(example, tfrec_format)\n","    return example['image'], example['image_name'] if return_image_name else 0\n","\n","\n","def prepare_image(img):    \n","    img = tf.image.decode_jpeg(img, channels=3)\n","    img = tf.cast(img, tf.float32) / 255.0           \n","    return img\n","\n","\n","def count_data_items(filenames):\n","    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) \n","         for filename in filenames]\n","    return np.sum(n)"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["def Transform(\n","        dim=256,\n","        rot_mult=180.0,\n","        shear_mult=2.0,\n","        hzoom_mult=8.0,\n","        wzoom_mult=8.0,\n","        hshift_mult=8.0,\n","        wshift_mult=8.0):\n","    def _transform(image):\n","        # input image - is one image of size [dim,dim,3] not a batch of [b,dim,dim,3]\n","        # output - image randomly rotated, sheared, zoomed, and shifted\n","        XDIM = dim % 2  # fix for size 331\n","\n","        rot = rot_mult * tf.random.normal([1], dtype='float32')\n","        shr = shear_mult * tf.random.normal([1], dtype='float32')\n","        h_zoom = 1.0 + tf.random.normal([1], dtype='float32') / hzoom_mult\n","        w_zoom = 1.0 + tf.random.normal([1], dtype='float32') / wzoom_mult\n","        h_shift = hshift_mult * tf.random.normal([1], dtype='float32')\n","        w_shift = wshift_mult * tf.random.normal([1], dtype='float32')\n","\n","        # GET TRANSFORMATION MATRIX\n","        m = _get_mat(rot, shr, h_zoom, w_zoom, h_shift, w_shift)\n","\n","        # LIST DESTINATION PIXEL INDICES\n","        x = tf.repeat(tf.range(dim // 2, -dim // 2, -1), dim)\n","        y = tf.tile(tf.range(-dim // 2, dim // 2), [dim])\n","        z = tf.ones([dim * dim], dtype='int32')\n","        idx = tf.stack([x, y, z])\n","\n","        # ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS\n","        idx2 = K.dot(m, tf.cast(idx, dtype='float32'))\n","        idx2 = K.cast(idx2, dtype='int32')\n","        idx2 = K.clip(idx2, -dim // 2 + XDIM + 1, dim // 2)\n","\n","        # FIND ORIGIN PIXEL VALUES\n","        idx3 = tf.stack([dim // 2 - idx2[0, ], dim // 2 - 1 + idx2[1, ]])\n","        d = tf.gather_nd(image, tf.transpose(idx3))\n","\n","        return tf.reshape(d, [dim, dim, 3])\n","\n","    return _transform\n","\n","def base_aug(img):\n","    img = tf.image.random_flip_left_right(img)\n","    #img = tf.image.random_hue(img, 0.01)\n","    img = tf.image.random_saturation(img, 0.7, 1.3)\n","    img = tf.image.random_contrast(img, 0.8, 1.2)\n","    img = tf.image.random_brightness(img, 0.1)\n","    return img\n","\n","\n","\n","transform_aug = Transform(dim=IMG_READ_SIZE)\n","\n","\n","def basic_augmentation_pipeline(ds: tf.data.Dataset, dim=None, batch_size=None) -> tf.data.Dataset:\n","    ds = ds.map(lambda i, o: (transform_aug(i), o), num_parallel_calls=AUTO)\n","    ds = ds.map(lambda i, o: (base_aug(i), o), num_parallel_calls=AUTO)\n","    return ds"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["def get_dataset(files, augment=False, repeat=False, shuffle=False, labeled=True, batch_size=16, drop_remainder=False, \n","                dim=256, read_dim=None\n","               ) -> tf.data.Dataset:\n","    if read_dim is None:\n","        read_dim = dim\n","    \n","    ds = tf.data.TFRecordDataset(files, num_parallel_reads=AUTO)\n","    ds = ds.cache()\n","    \n","    if repeat:\n","        ds = ds.repeat()\n","    \n","    if shuffle: \n","        ds = ds.shuffle(1024 * 8)\n","        opt = tf.data.Options()\n","        opt.experimental_deterministic = False\n","        ds = ds.with_options(opt)\n","        \n","    if labeled: \n","        ds = ds.map(read_labeled_tfrecord, num_parallel_calls=AUTO)\n","    else:\n","        ds = ds.map(read_unlabeled_tfrecord, num_parallel_calls=AUTO)\n","    ds = ds.map(lambda i, o: (prepare_image(i), o), num_parallel_calls=AUTO)\n","\n","    if augment:\n","        ds = basic_augmentation_pipeline(ds, batch_size=8 * batch_size, dim=read_dim) \n","        if isinstance(augment, list):\n","            for a in augment:\n","                ds = ds.map(a, num_parallel_calls=AUTO)\n","        \n","    ds = ds.map(lambda i, o: (tf.image.resize(i, [dim, dim]), o), num_parallel_calls=AUTO)\n","    ds = ds.batch(batch_size, drop_remainder=drop_remainder)\n","    ds = ds.prefetch(AUTO)\n","    return ds\n","\n","def separate_by_target(ds: tf.data.Dataset, idx: int = 1, thr: float = 0.5\n","                       ) -> typing.Tuple[tf.data.Dataset, tf.data.Dataset]:\n","    def _cond0(*args):\n","        return tf.cast(args[idx], tf.float32) < thr\n","\n","    def _cond1(*args):\n","        return tf.cast(args[idx], tf.float32) >= thr\n","\n","    ds0 = ds.filter(_cond0)\n","    ds1 = ds.filter(_cond1)\n","\n","    return ds0, ds1\n","\n","\n","def merge_dataset(ds0: tf.data.Dataset, ds1: tf.data.Dataset, pos_ratio: float = 0.5) -> tf.data.Dataset:\n","    n1 = int(1000 * pos_ratio)\n","    n0 = 1000 - n1\n","    choice_ds = tf.data.Dataset.from_tensor_slices(\n","        np.concatenate([np.zeros(n0), np.ones(n1)]).astype('int64')).shuffle(n0 + n1).repeat()\n","    ds = tf.data.experimental.choose_from_datasets([ds0, ds1],\n","                                                   choice_ds.prefetch(tf.data.experimental.AUTOTUNE))\n","    return ds\n","\n","def get_balanced_dataset(files, augment=False, repeat=False, shuffle=False, batch_size=16, drop_remainder=False, \n","                         dim=256, read_dim=None, pos_ratio=False) -> tf.data.Dataset:\n","    if read_dim is None:\n","        read_dim = dim\n","    \n","    ds = tf.data.TFRecordDataset(files, num_parallel_reads=AUTO)\n","    ds = ds.cache()\n","    \n","    if repeat:\n","        ds = ds.repeat()\n","    \n","    ds = ds.map(read_labeled_tfrecord, num_parallel_calls=AUTO)\n","        \n","    ds0, ds1 = balance.separate_by_target(ds)\n","    ds0 = ds.map(lambda i, o: (prepare_image(i), o), num_parallel_calls=AUTO)\n","    ds1 = ds.map(lambda i, o: (prepare_image(i), o), num_parallel_calls=AUTO)\n","    ds = merge_dataset(ds0, ds1, pos_ratio)\n","    if shuffle: \n","        ds = ds.shuffle(1024)\n","        opt = tf.data.Options()\n","        opt.experimental_deterministic = False\n","        ds = ds.with_options(opt)\n","    \n","    if augment:\n","        ds = basic_augmentation_pipeline(ds, batch_size=8 * batch_size, dim=read_dim) \n","        if isinstance(augment, list):\n","            for a in augment:\n","                ds = ds.map(a, num_parallel_calls=AUTO)\n","    ds = ds.map(lambda i, o: (tf.image.resize(i, [dim, dim]), o), num_parallel_calls=AUTO)\n","        \n","    ds = ds.batch(batch_size, drop_remainder=drop_remainder)\n","    ds = ds.prefetch(AUTO)\n","    return ds"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["## Model building"]},{"metadata":{"trusted":true},"cell_type":"code","source":["from tensorflow.keras import Model\n","from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization, GlobalAveragePooling2D, Activation\n","from tensorflow.keras.optimizers import Adam, SGD\n","from tensorflow.keras.losses import BinaryCrossentropy\n","def build_model_batchnormalization(dim=128, ef=0):\n","    inp = Input(shape=(dim,dim,3))\n","    base = getattr(efnet, \n","                   'EfficientNetB%d' % ef)(input_shape=(dim, dim, 3), \n","                   weights='imagenet', \n","                   include_top=False)\n","    x = base(inp)\n","    x = GlobalAveragePooling2D()(x)\n","    x = BatchNormalization()(x)\n","    x = Dense(512,activation='relu')(x)\n","    x = BatchNormalization()(x)\n","    x = Dense(256,activation='relu')(x)\n","    x = BatchNormalization()(x)\n","    x = Dense(256,activation='relu')(x)\n","    x = Dropout(0.3)(x)\n","    x = Dense(128,activation='relu')(x)\n","    x = Dropout(0.3)(x)\n","    x = Dense(1)(x)\n","    x = Activation('sigmoid', dtype='float32')(x)\n","    model = Model(inputs=inp,outputs=x)\n","    opt = Adam(learning_rate=1e-3)\n","#     opt = SGD(learning_rate=1e-3)\n","\n","    loss = BinaryCrossentropy(**LOSS_PARAMS)\n","\n","    model.compile(optimizer=opt, loss=loss, metrics=['AUC'])\n","    return model\n","def build_model(dim=128, ef=0):\n","    inp = Input(shape=(dim,dim,3))\n","    base = getattr(efnet, \n","                   'EfficientNetB%d' % ef)(input_shape=(dim, dim, 3), \n","                   weights='imagenet', \n","                   include_top=False)\n","    x = base(inp)\n","    x = GlobalAveragePooling2D()(x)\n","    x = BatchNormalization()(x)\n","    x = Dense(512,activation='relu')(x)\n","    x = BatchNormalization()(x)\n","    x = Dense(256,activation='relu')(x)\n","    x = Dropout(0.4)(x)\n","    x = Dense(256,activation='relu')(x)\n","    x = Dropout(0.3)(x)\n","    x = Dense(128,activation='relu')(x)\n","    x = Dropout(0.3)(x)\n","    x = Dense(1)(x)\n","    x = Activation('sigmoid', dtype='float32')(x)\n","    model = Model(inputs=inp,outputs=x)\n","    opt = Adam(learning_rate=1e-3)\n","#     opt = SGD(learning_rate=1e-3)\n","    loss = BinaryCrossentropy(**LOSS_PARAMS)\n","    model.compile(optimizer=opt, loss=loss, metrics=['AUC'])\n","    return model"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["# build checkpoint folder\n","CKPT_FOLDER = \"../working/ckpt\"\n","if not os.path.exists(CKPT_FOLDER):\n","    os.mkdir(CKPT_FOLDER)\n","# build folds\n","folds = list(model_selection.KFold(n_splits=FOLDS, shuffle=True, random_state=SEED).split(np.arange(15)))\n","testiness = pd.read_csv(\"../input/spicv-spicy-vi-make-your-cv-more-testy/testiness.csv\")\n","#\n","TOTAL_POS = 581 + 2858 * INCLUDE_2019 + 1651 * INCLUDE_2018 + 580 * INCLUDE_MALIGNANT"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["VERBOSE = 1\n","PLOT    = 1\n","\n","histories = []\n","df_oof = pd.DataFrame(); df_res = pd.DataFrame()\n","t_start = time.time()\n","for fold, (idTrain, idValid) in enumerate(folds):\n","    print(\"#\" * 68)\n","    print((\"#\" * 20 + \"\\t\\tFold %d\\t\\t\" + \"#\" * 20) % fold)\n","    print(\"#\" * 68)\n","    # prepare TPU\n","    if DEVICE == 'TPU':\n","        if tpu: \n","            tf.tpu.experimental.initialize_tpu_system(tpu)\n","    # build fold train-valid split   \n","    fold_valid_files = [f for f in train_files if any([int(re.match(\"^train([0-9]+)\", f.split(\"/\")[-1]).group(1)) % 15 == i for i in idValid])]\n","    fold_valid_files = [f for f in fold_valid_files if GCS_PATH1 in f] # only data from the original dataset\n","    # fold_train_files = [f for f in train_files if any([int(re.match(\"^train([0-9]+)\", f.split(\"/\")[-1]).group(1)) % 15 == i for i in idTrain])]\n","    fold_train_files = [f for f in train_files if f not in fold_valid_files]\n","    np.random.shuffle(fold_train_files)\n","    print(\"Train files: %d\\t\\t Valid files: %d\" % (len(fold_train_files), len(fold_valid_files)))\n","    # build model and set precision policy\n","    K.clear_session()   \n","    if DEVICE == 'TPU':\n","        keras.mixed_precision.experimental.set_policy('mixed_bfloat16')\n","    with strategy.scope():\n","        model = build_model(dim=IMG_SIZE, ef=EFF_NET)\n","   \n","    # build training dataset\n","    print(\"Using balanced dataset with pos_ratio = %d%%\" % int(100 * BALANCE_POS_RATIO))\n","    ds_train = get_balanced_dataset(fold_train_files, repeat=True,  augment=True,  drop_remainder=True,                                                    shuffle=True,  \n","                                        pos_ratio=BALANCE_POS_RATIO,\n","                                        dim=IMG_SIZE, read_dim=IMG_READ_SIZE, batch_size=GLOBAL_BATCH_SIZE)\n","    FOLD_POS = TOTAL_POS * (FOLDS - 1) / FOLDS\n","    STEPS = int(FOLD_POS / BALANCE_POS_RATIO / GLOBAL_BATCH_SIZE)\n","\n","\n","\n","    ds_valid = get_dataset(fold_valid_files, repeat=False, augment=False, drop_remainder=False, shuffle=False, \n","                           dim=IMG_SIZE, read_dim=IMG_READ_SIZE, batch_size=GLOBAL_BATCH_SIZE * TBM)\n","    # callbacks\n","    FOLD_CKPT_FOLDER = os.path.join(CKPT_FOLDER, \"fold%d\" % fold)\n","    if not os.path.exists(FOLD_CKPT_FOLDER):\n","        os.mkdir(FOLD_CKPT_FOLDER)\n","    callbacks =[\n","        keras.callbacks.ModelCheckpoint(os.path.join(FOLD_CKPT_FOLDER, \"model_fold%d_e{epoch:02d}.h5\" % fold),                                             save_weights_only=True,\n","                                       save_freq= int(STEPS*3))]    \n","    # train\n","    \n","    print(\"Training...\")\n","    history = model.fit(\n","                            ds_train,\n","        validation_data   = ds_valid,\n","        epochs            = EPOCHS,\n","        steps_per_epoch   = STEPS,\n","        verbose           = VERBOSE,\n","        callbacks         = callbacks,\n","        validation_freq   = VALID_FREQ\n","    )\n","    histories.append(history)    \n","    \n","    # SWA\n","    ckpt_files = np.sort(tf.io.gfile.glob(os.path.join(FOLD_CKPT_FOLDER, \"*.h5\")))\n","\n","    for f in ckpt_files:\n","        os.remove(f)\n","    model.save(os.path.join(CKPT_FOLDER, \"model_fold%d.h5\" % fold))\n","    # VALID\n","    ds_valid = get_dataset(fold_valid_files, augment=TTA >= 1, repeat=True, dim=IMG_SIZE, read_dim=IMG_READ_SIZE, batch_size=GLOBAL_BATCH_SIZE * TBM, drop_remainder=True)\n","    ct_valid = count_data_items(fold_valid_files); STEPS = int(np.ceil(TTA * ct_valid / GLOBAL_BATCH_SIZE / TBM))\n","    fold_valid_pred = model.predict(ds_valid, steps=STEPS, verbose=1)\n","    fold_valid_pred = fold_valid_pred[:ct_valid * TTA,]\n","    ds_valid = get_dataset(fold_valid_files, augment=False, repeat=False, dim=IMG_SIZE, batch_size=GLOBAL_BATCH_SIZE * TBM, drop_remainder=False, labeled=False)\n","    fold_valid_names = np.concatenate([np.array([ni.decode(\"utf-8\") for ni in n.numpy()]) for n in ds_valid.map(lambda i, n: n)], 0)\n","    \n","    fold_df = pd.DataFrame({'image_name': np.tile(fold_valid_names, [TTA]), 'pred': fold_valid_pred.squeeze(), 'fold': fold})\n","    df_oof = pd.concat([df_oof, fold_df])\n","    fold_df['image_name'] = fold_df['image_name'].str.replace('_downsampled', '')\n","    fold_df = fold_df.groupby('image_name').mean().reset_index()\n","    fold_df = fold_df.merge(df_base_train[['image_name', 'patient_id', 'target']], on='image_name').merge(testiness, on='image_name')\n","    fold_df['fold'] = fold\n","    auc  = metrics.roc_auc_score(fold_df.target, fold_df.pred)\n","    \n","    # TEST\n","    ds_test = get_dataset(test_files, augment=TTA >= 1, repeat=True, dim=IMG_SIZE, read_dim=IMG_READ_SIZE, batch_size=GLOBAL_BATCH_SIZE * TBM, drop_remainder=True, labeled=False)\n","    ct_test = count_data_items(test_files); STEPS = int(np.ceil(TTA * ct_test / GLOBAL_BATCH_SIZE / TBM))\n","    fold_test_pred = model.predict(ds_test.map(lambda i, l: i), steps=STEPS, verbose=1)\n","    fold_test_pred = fold_test_pred[:ct_test * TTA,]\n","    ds_test = get_dataset(test_files, augment=False, repeat=False, dim=IMG_SIZE, batch_size=GLOBAL_BATCH_SIZE * TBM, drop_remainder=False, labeled=False)\n","    fold_test_names = np.concatenate([np.array([ni.decode(\"utf-8\") for ni in n.numpy()]) for n in ds_test.map(lambda i, n: n)], 0)\n","    \n","    fold_res = pd.DataFrame({'image_name': np.tile(fold_test_names, [TTA]), 'pred': fold_test_pred.squeeze(), 'fold': fold})\n","    df_res = pd.concat([df_res, fold_res])\n","    \n","    # time\n","    used_time_till_now = time.time() - t_start\n","    time_per_fold = used_time_till_now / (fold + 1)\n","    print(\"Validation AUC last epoch = %.4f\" % history.history['val_auc'][-1])\n","    print(\"Validation AUC  (TTA %2d) = %.4f\" % (TTA, auc))\n","    print(\"Total time = %ds\\t\\tTime per fold = %ds\" % (int(used_time_till_now), int(time_per_fold)))\n","    \n","    # plot\n","    if PLOT:\n","        plt.figure(figsize=(16, 4))\n","        plt.subplot(1, 2, 1)\n","        plt.plot(history.history['loss'], color='tab:blue', marker='o')\n","        plt.plot(range(VALID_FREQ - 1, EPOCHS, VALID_FREQ), history.history['val_loss'], color='tab:blue', marker='x', linestyle=':')\n","        plt.yscale('log')\n","        plt.subplot(1, 2, 2)\n","        plt.plot(history.history['auc'], color='tab:red', marker='o')\n","        plt.plot(range(VALID_FREQ - 1, EPOCHS, VALID_FREQ), history.history['val_auc'], color='tab:red', marker='x', linestyle=':')\n","        plt.show()\n","    \n","    del model, ds_train, ds_valid, ds_test\n","    print(\"\\n\\n\")\n","    \n"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["avgh = dict()\n","for history in histories:\n","    for k in history.history.keys():\n","        if k in avgh.keys():\n","            avgh[k] = np.concatenate([avgh[k], np.array(history.history[k]).reshape(-1, 1)], 1)\n","        else:\n","            avgh[k] = np.array(history.history[k]).reshape(-1, 1)\n","plt.figure(figsize=(16, 4))\n","plt.subplot(1, 2, 1)\n","plt.title('Log Loss')\n","plt.plot(avgh['loss'], marker='o', color='tab:blue', alpha=0.2)\n","plt.plot(avgh['loss'].mean(1), color='tab:blue')\n","plt.plot(range(VALID_FREQ - 1, EPOCHS, VALID_FREQ), avgh['val_loss'], color='tab:blue', alpha=0.2, linestyle=\":\")\n","plt.plot(range(VALID_FREQ - 1, EPOCHS, VALID_FREQ), avgh['val_loss'].mean(1), marker='x', color='tab:blue', linestyle=\":\")\n","plt.yscale('log')\n","plt.subplot(1, 2, 2)\n","plt.title('AUC')\n","plt.plot(avgh['auc'], marker='o', color='tab:red', alpha=0.2)\n","plt.plot(avgh['auc'].mean(1), color='tab:red')\n","plt.plot(range(VALID_FREQ - 1, EPOCHS, VALID_FREQ), avgh['val_auc'], color='tab:red', alpha=0.2, linestyle=\":\")\n","plt.plot(range(VALID_FREQ - 1, EPOCHS, VALID_FREQ), avgh['val_auc'].mean(1), marker='x', color='tab:red', linestyle=\":\");"],"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.7.4-final","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}